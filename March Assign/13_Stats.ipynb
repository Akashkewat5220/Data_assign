{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d31c6229-70ee-4363-9cc5-75b2e6bbfa4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa1d8822-ad1b-408c-b1cf-a4bbe8b7370e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The assumptions required to use ANOVA are:\\n\\nIndependence: The observations within each group are independent of each other.\\nHomogeneity of Variance: The variances of the dependent variable are equal across all groups.\\nNormality: The distribution of the dependent variable is approximately normal within each group.\\nViolations of these assumptions can impact the validity of ANOVA results. For example:\\n\\nViolation of independence: If observations within a group are correlated, it can lead to biased results and incorrect conclusions.\\nViolation of homogeneity of variance: Unequal variances between groups can affect the F-statistic and lead to incorrect significance testing.\\nViolation of normality: If the data are not approximately normally distributed within each group, the results may not be reliable, especially for smaller sample sizes.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The assumptions required to use ANOVA are:\n",
    "\n",
    "Independence: The observations within each group are independent of each other.\n",
    "Homogeneity of Variance: The variances of the dependent variable are equal across all groups.\n",
    "Normality: The distribution of the dependent variable is approximately normal within each group.\n",
    "Violations of these assumptions can impact the validity of ANOVA results. For example:\n",
    "\n",
    "Violation of independence: If observations within a group are correlated, it can lead to biased results and incorrect conclusions.\n",
    "Violation of homogeneity of variance: Unequal variances between groups can affect the F-statistic and lead to incorrect significance testing.\n",
    "Violation of normality: If the data are not approximately normally distributed within each group, the results may not be reliable, especially for smaller sample sizes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2e3aa-8f63-432d-93df-20519fe8d44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "696be75d-975b-490d-8997-eb2cd5fe0fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa81498f-64e0-40ee-86f5-c06993b5e9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The three types of ANOVA are:\\n\\nOne-Way ANOVA: It is used when there is only one factor being tested, and it compares the means of two or more groups.\\nTwo-Way ANOVA: It is used when there are two factors being tested, and it examines the main effects of each factor and their interaction effect.\\nThree-Way ANOVA: It is used when there are three factors being tested, examining their main effects and possible interaction effects.\\nThe choice of ANOVA type depends on the design and research question of the study.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The three types of ANOVA are:\n",
    "\n",
    "One-Way ANOVA: It is used when there is only one factor being tested, and it compares the means of two or more groups.\n",
    "Two-Way ANOVA: It is used when there are two factors being tested, and it examines the main effects of each factor and their interaction effect.\n",
    "Three-Way ANOVA: It is used when there are three factors being tested, examining their main effects and possible interaction effects.\n",
    "The choice of ANOVA type depends on the design and research question of the study.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a331e0-6e32-42b4-97c0-a54c84afcc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dcfb1fd-6310-476c-bae6-5e4f5a5082d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49ac0113-3bce-46ed-826c-b7ee254bba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The partitioning of variance in ANOVA refers to the decomposition of the total variance in the dependent variable into different components associated with different sources of variation. It is important to understand this concept because it allows us to quantify the amount of variance explained by factors of interest and assess their significance. By understanding how the total variance is partitioned, we can determine the relative contributions of different factors and evaluate their impact on the dependent variable. '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The partitioning of variance in ANOVA refers to the decomposition of the total variance in the dependent variable into different components associated with different sources of variation. It is important to understand this concept because it allows us to quantify the amount of variance explained by factors of interest and assess their significance. By understanding how the total variance is partitioned, we can determine the relative contributions of different factors and evaluate their impact on the dependent variable. \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddb859-36c7-4876-8e3a-53d7dc674711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78b4cde1-08ca-400b-b040-2c904839dfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa9d0cfe-cf3c-4d11-8ebf-93304159bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Group data\n",
    "group1 = [2, 3, 4]\n",
    "group2 = [1, 2, 3]\n",
    "group3 = [3, 4, 5]\n",
    "\n",
    "# Calculate ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(group1, group2, group3)\n",
    "\n",
    "# Total sum of squares (SST)\n",
    "sst = np.sum((np.concatenate([group1, group2, group3]) - np.mean(np.concatenate([group1, group2, group3])))**2)\n",
    "\n",
    "# Explained sum of squares (SSE)\n",
    "sse = np.sum((np.mean(group1) - np.mean(np.concatenate([group1, group2, group3])))**2) * len(group1)\n",
    "sse += np.sum((np.mean(group2) - np.mean(np.concatenate([group1, group2, group3])))**2) * len(group2)\n",
    "sse += np.sum((np.mean(group3) - np.mean(np.concatenate([group1, group2, group3])))**2) * len(group3)\n",
    "\n",
    "# Residual sum of squares (SSR)\n",
    "ssr = sst - sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf431b-3e6d-453c-a3f4-ad100b72b1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a57694a-89e9-42ae-984f-1070dfcf0e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0e8622c-1ab8-4ca0-ba74-06de1573427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create the design matrix\n",
    "df = pd.DataFrame({'A': [1, 1, 1, 2, 2, 2],\n",
    "                   'B': [1, 2, 3, 1, 2, 3],\n",
    "                   'Y': [4, 5, 6, 7, 8, 9]})\n",
    "\n",
    "# Fit the model\n",
    "model = ols('Y ~ A + B + A:B', data=df).fit()\n",
    "\n",
    "# Calculate main effects\n",
    "main_effects = model.params[['A', 'B']]\n",
    "\n",
    "# Calculate interaction effect\n",
    "interaction_effect = model.params['A:B']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a3934-fbee-4932-9334-8d742f676b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e58bdff3-8456-4738-85e2-e5ed57a5ec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12fe2eff-a807-4092-8ecd-079475d92baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' With an F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA, we can conclude that there are significant differences between the groups. The F-statistic indicates that the variation between the group means is larger than the variation within the groups. The p-value of 0.02 suggests that the probability of observing such a difference by chance, assuming no actual difference between the groups, is 2%. Therefore, we reject the null hypothesis of equal group means and conclude that there are statistically significant differences between the groups.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" With an F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA, we can conclude that there are significant differences between the groups. The F-statistic indicates that the variation between the group means is larger than the variation within the groups. The p-value of 0.02 suggests that the probability of observing such a difference by chance, assuming no actual difference between the groups, is 2%. Therefore, we reject the null hypothesis of equal group means and conclude that there are statistically significant differences between the groups.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0206d-d5e4-4cb3-a6da-75187f859d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ceff023-198d-4f54-9f69-272b1a155e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcec73d7-f1a0-4270-a332-e26f25dd0bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Handling missing data in a repeated measures ANOVA can be done using various methods such as pairwise deletion, mean substitution, or multiple imputation. The consequences of using different methods include potential biases, loss of statistical power, and inaccurate estimates of the effects.\\n\\nPairwise deletion: It involves excluding cases with missing data for specific comparisons. It can lead to biased estimates and reduced statistical power if missingness is not completely random.\\nMean substitution: Missing values are replaced with the mean of the available data. It assumes that missing values are missing completely at random (MCAR) and can lead to biased estimates and underestimation of variability.\\nMultiple imputation: It involves estimating missing values using statistical techniques. It can provide unbiased estimates if missingness is handled properly, but it requires assumptions about the missing data mechanism.\\nChoosing an appropriate method depends on the nature of missing data and the assumptions made about the missing data mechanism.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Handling missing data in a repeated measures ANOVA can be done using various methods such as pairwise deletion, mean substitution, or multiple imputation. The consequences of using different methods include potential biases, loss of statistical power, and inaccurate estimates of the effects.\n",
    "\n",
    "Pairwise deletion: It involves excluding cases with missing data for specific comparisons. It can lead to biased estimates and reduced statistical power if missingness is not completely random.\n",
    "Mean substitution: Missing values are replaced with the mean of the available data. It assumes that missing values are missing completely at random (MCAR) and can lead to biased estimates and underestimation of variability.\n",
    "Multiple imputation: It involves estimating missing values using statistical techniques. It can provide unbiased estimates if missingness is handled properly, but it requires assumptions about the missing data mechanism.\n",
    "Choosing an appropriate method depends on the nature of missing data and the assumptions made about the missing data mechanism.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c361e2e-211c-41d8-897a-979343ec8ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a60034ef-1809-4d8d-bd47-d8b83b8453e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7acfb04-5dd5-41b1-9fb7-f55792db4192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Common post-hoc tests used after ANOVA include Tukey's Honestly Significant Difference (HSD), Bonferroni correction, and Scheffe's test.\\n\\nTukey's HSD: It is used to identify which specific group means differ significantly from each other. It controls the familywise error rate and is suitable when performing multiple pairwise comparisons.\\nBonferroni correction: It adjusts the significance level to maintain the overall alpha level when performing multiple comparisons. It is more conservative and controls the familywise error rate.\\nScheffe's test: It allows for complex comparisons and is more robust against violations of assumptions. It controls the familywise error rate but has lower power compared to other tests.\\nPost-hoc tests are necessary when an ANOVA indicates a significant overall effect but does not provide information about specific pairwise comparisons. For example, in a study comparing the effectiveness of three different treatment groups, a post-hoc test can determine which groups significantly differ from each other. \""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Common post-hoc tests used after ANOVA include Tukey's Honestly Significant Difference (HSD), Bonferroni correction, and Scheffe's test.\n",
    "\n",
    "Tukey's HSD: It is used to identify which specific group means differ significantly from each other. It controls the familywise error rate and is suitable when performing multiple pairwise comparisons.\n",
    "Bonferroni correction: It adjusts the significance level to maintain the overall alpha level when performing multiple comparisons. It is more conservative and controls the familywise error rate.\n",
    "Scheffe's test: It allows for complex comparisons and is more robust against violations of assumptions. It controls the familywise error rate but has lower power compared to other tests.\n",
    "Post-hoc tests are necessary when an ANOVA indicates a significant overall effect but does not provide information about specific pairwise comparisons. For example, in a study comparing the effectiveness of three different treatment groups, a post-hoc test can determine which groups significantly differ from each other. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c550d-8f0e-4e1c-ba6b-5c3c4afdf25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31f248f7-a6ed-4627-acda-413c6179f82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eadc64dd-6c64-4ae1-ba60-ba3b086f0e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m diet_B \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.8\u001b[39m, \u001b[38;5;241m2.9\u001b[39m, \u001b[38;5;241m2.7\u001b[39m, \u001b[38;5;241m2.3\u001b[39m, \u001b[38;5;241m2.1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])  \u001b[38;5;66;03m# Weight loss data for diet B\u001b[39;00m\n\u001b[1;32m      6\u001b[0m diet_C \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m3.5\u001b[39m, \u001b[38;5;241m2.8\u001b[39m, \u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m2.6\u001b[39m, \u001b[38;5;241m2.9\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])  \u001b[38;5;66;03m# Weight loss data for diet C\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m f_statistic, p_value \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_oneway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiet_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiet_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiet_C\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF-statistic:\u001b[39m\u001b[38;5;124m\"\u001b[39m, f_statistic)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value:\u001b[39m\u001b[38;5;124m\"\u001b[39m, p_value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/stats/_stats_py.py:3857\u001b[0m, in \u001b[0;36mf_oneway\u001b[0;34m(axis, *samples)\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(samples) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least two inputs are required;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3855\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3857\u001b[0m samples \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray(sample, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m   3859\u001b[0m \u001b[38;5;66;03m# ANOVA on N groups, each in its own array\u001b[39;00m\n\u001b[1;32m   3860\u001b[0m num_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(samples)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/stats/_stats_py.py:3857\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(samples) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least two inputs are required;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3855\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3857\u001b[0m samples \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m   3859\u001b[0m \u001b[38;5;66;03m# ANOVA on N groups, each in its own array\u001b[39;00m\n\u001b[1;32m   3860\u001b[0m num_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(samples)\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "diet_A = np.array([2.1, 3.2, 1.9, 2.5, 3.1, ...])  # Weight loss data for diet A\n",
    "diet_B = np.array([1.8, 2.9, 2.7, 2.3, 2.1, ...])  # Weight loss data for diet B\n",
    "diet_C = np.array([3.5, 2.8, 3.0, 2.6, 2.9, ...])  # Weight loss data for diet C\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a36f3-7da0-4482-8513-733a9083dd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0f4b9-35e9-43b7-9a95-06406eb22233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572022e4-c359-426e-94ea-9cfbf8b4c4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6360c-1f85-47c4-a1a3-8d5fc958536b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f32961-6825-40f8-9dcd-d487c814a22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
